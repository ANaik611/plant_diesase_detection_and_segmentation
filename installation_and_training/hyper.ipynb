{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch Size\n",
    "The **batch size** is the number of training samples processed before the model's internal parameters are updated. It influences both the training dynamics and the resource requirements.\n",
    "\n",
    "- **Small Batch Size**:\n",
    "  - Requires less memory, allowing for training on lower-end hardware.\n",
    "  - Introduces noise in the gradient updates, which may help the model generalize better but may also slow down convergence.\n",
    "  \n",
    "- **Large Batch Size**:\n",
    "  - Provides more accurate gradient estimates, leading to smoother updates and faster convergence.\n",
    "  - Requires more memory and might lead to overfitting or poor generalization due to less noisy updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo segment train model=yolov8s-seg.pt data=/content/drive/MyDrive/DATASET_NAME/data.yaml epochs=30 imgsz=640 batch=16 project=/content/drive/MyDrive/results name=batch_16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Learning Rate\n",
    "The **learning rate** controls how large of a step the optimizer takes in the direction of the gradient during each update.\n",
    "\n",
    "- **Low Learning Rate**:\n",
    "  - Leads to slow convergence but may result in better long-term performance as it allows the optimizer to find more optimal weights.\n",
    "  \n",
    "- **High Learning Rate**:\n",
    "  - Speeds up training, but can result in the model skipping over optimal solutions, or even cause divergence if the learning rate is too high.\n",
    "\n",
    "Choosing the right learning rate is crucial for efficient training and avoiding overshooting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo segment train model=yolov8s-seg.pt data=/content/drive/MyDrive/DATASET_NAME/data.yaml epochs=30 imgsz=640 lr0=0.01 project=/content/drive/MyDrive/results name=lr_0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Momentum\n",
    "**Momentum** is a method used to accelerate gradient descent by considering past gradients. It helps smooth out the updates and avoid oscillations.\n",
    "\n",
    "- **High Momentum**:\n",
    "  - Increases the speed of convergence by accumulating previous gradients and continuing in their direction, leading to more stable updates.\n",
    "  \n",
    "- **Low Momentum**:\n",
    "  - Reduces the influence of previous gradients, potentially slowing down convergence.\n",
    "\n",
    "Momentum allows the model to build velocity in directions where the gradient is consistent, thus speeding up training in flatter areas of the loss landscape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo segment train model=yolov8s-seg.pt data=/content/drive/MyDrive/DATASET_NAME/data.yaml epochs=30 imgsz=640 momentum=0.937 project=/content/drive/MyDrive/results name=momentum_0.937\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weight Decay\n",
    "**Weight decay** is a regularization technique that adds a penalty to the loss function based on the magnitude of the model weights. This helps prevent the model from overfitting by keeping the weights small.\n",
    "\n",
    "- **High Weight Decay**:\n",
    "  - Strongly penalizes large weights, encouraging simpler models that generalize better but may underfit if too large.\n",
    "  \n",
    "- **Low Weight Decay**:\n",
    "  - Lightly penalizes the weights, reducing the risk of underfitting but potentially allowing the model to overfit to the training data.\n",
    "\n",
    "Weight decay is commonly used in conjunction with optimization techniques like SGD (Stochastic Gradient Descent) to balance the complexity of the model and its ability to generalize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo segment train model=yolov8s-seg.pt data=/content/drive/MyDrive/DATASET_NAME/data.yaml epochs=30 imgsz=640 weight_decay=0.0005 project=/content/drive/MyDrive/results name=weight_decay_0.0005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Each of these hyperparameters—**batch size**, **learning rate**, **momentum**, and **weight decay**—plays a critical role in training deep learning models. Proper tuning can significantly improve model performance, generalization, and convergence speed. Experimentation and validation are often required to find the optimal settings for a given model and dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
